{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4e3160",
   "metadata": {},
   "source": [
    "# Scaling Law Analysis (Bonus)\n",
    "This notebook fits a simple scaling law to the provided hypothetical training results, predicts loss for a larger run, and recommends a compute-optimal allocation under a fixed compute budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"model_params\": [1e8, 3e8, 1e9, 3e9],\n",
    "    \"dataset_tokens\": [1e10, 3e10, 1e11, 3e11],\n",
    "    \"compute_pf_days\": [0.1, 0.5, 2.0, 10.0],\n",
    "    \"final_loss\": [2.50, 2.10, 1.75, 1.50],\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82df277",
   "metadata": {},
   "source": [
    "## 1) Fit a scaling law\n",
    "Because model size and dataset size scale together almost perfectly in the table (tokens ≈ 100×parameters), it is difficult to uniquely identify separate exponents for model-size and data-size effects. A robust choice with this data is to fit a compute-scaling law:\n",
    "\n",
    "\\[ L(C) = L_\\infty + K\\,C^{-k} \\]\n",
    "\n",
    "where \\(C\\) is compute in PF-days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = data[\"compute_pf_days\"].to_numpy()\n",
    "L = data[\"final_loss\"].to_numpy()\n",
    "\n",
    "def loss_vs_compute(C, L_inf, K, k):\n",
    "    return L_inf + K * (C ** (-k))\n",
    "\n",
    "params, cov = curve_fit(\n",
    "    loss_vs_compute,\n",
    "    C, L,\n",
    "    p0=[0.3, 1.5, 0.15],\n",
    "    bounds=([0.0, 0.0, 0.0], [5.0, 10.0, 2.0]),\n",
    "    maxfev=200000\n",
    ")\n",
    "\n",
    "L_inf, K, k = params\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: loss vs compute (log-x)\n",
    "C_grid = np.logspace(np.log10(C.min()), np.log10(C.max()), 200)\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.scatter(C, L)\n",
    "plt.plot(C_grid, loss_vs_compute(C_grid, *params))\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Compute (PF-days, log scale)\")\n",
    "plt.ylabel(\"Final loss\")\n",
    "plt.title(\"Compute scaling fit: L(C) = L_inf + K*C^{-k}\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Fitted parameters: L_inf={L_inf:.3f}, K={K:.3f}, k={k:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95b66a",
   "metadata": {},
   "source": [
    "## 2) Predict expected loss for a 10B parameter model trained on 1T tokens\n",
    "To use the compute-scaling fit, we need an estimated compute for (10B params, 1T tokens). The provided table implicitly uses a fixed ratio of tokens to parameters:\n",
    "\n",
    "\\[ \\text{tokens} \\approx 100\\times \\text{parameters} \\]\n",
    "\n",
    "We fit compute as a power-law function of model size along this frontier:\n",
    "\n",
    "\\[ C(N) = c_0\\,N^{p} \\]\n",
    "\n",
    "and then infer compute for 10B params (and 1T tokens) under the same scaling regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = data[\"model_params\"].to_numpy()\n",
    "D = data[\"dataset_tokens\"].to_numpy()\n",
    "\n",
    "# Verify frontier ratio\n",
    "ratio = D / N\n",
    "print(\"Tokens/parameter ratios:\", ratio)\n",
    "\n",
    "# Fit compute as a function of N (since D scales ~100N here)\n",
    "logN = np.log(N)\n",
    "logC = np.log(C)\n",
    "p, logc0 = np.polyfit(logN, logC, 1)\n",
    "c0 = np.exp(logc0)\n",
    "\n",
    "print(f\"Compute fit: C(N) = {c0:.3e} * N^{p:.3f}\")\n",
    "\n",
    "def compute_from_N(N):\n",
    "    return c0 * (N ** p)\n",
    "\n",
    "# Prediction target\n",
    "N_target = 10e9   # 10B parameters\n",
    "D_target = 1e12   # 1T tokens (matches ~100 tokens/param)\n",
    "C_target = compute_from_N(N_target)\n",
    "\n",
    "L_target = loss_vs_compute(C_target, *params)\n",
    "\n",
    "C_target, L_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.scatter(N, C, label=\"data\")\n",
    "N_grid = np.logspace(np.log10(N.min()), np.log10(N.max()), 200)\n",
    "plt.plot(N_grid, compute_from_N(N_grid), label=\"fit\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Model parameters (log scale)\")\n",
    "plt.ylabel(\"Compute (PF-days, log scale)\")\n",
    "plt.title(\"Compute vs model size along the provided scaling frontier\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estimated compute for 10B params / 1T tokens: {C_target:.1f} PF-days\")\n",
    "print(f\"Predicted loss at that compute: {L_target:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8dc33",
   "metadata": {},
   "source": [
    "## 3) Recommend an optimal allocation under a fixed compute budget (20 PF-days)\n",
    "Given this dataset follows a near-fixed frontier (tokens ≈ 100×params), a practical compute-optimal recommendation under a fixed compute budget is to stay on the same frontier and pick the largest model/data pair that fits the budget.\n",
    "\n",
    "Using the fitted compute-vs-size relation, we solve for \\(N\\) such that \\(C(N)=20\\) PF-days, then set \\(D\\approx 100N\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 20.0  # PF-days\n",
    "\n",
    "# Solve C(N) = budget => N = (budget/c0)^(1/p)\n",
    "N_budget = (budget / c0) ** (1.0 / p)\n",
    "D_budget = 100.0 * N_budget  # follow the observed frontier ratio\n",
    "\n",
    "L_budget = loss_vs_compute(budget, *params)\n",
    "\n",
    "print(f\"Compute budget: {budget} PF-days\")\n",
    "print(f\"Recommended (frontier) model size: {N_budget/1e9:.2f}B params\")\n",
    "print(f\"Recommended (frontier) dataset size: {D_budget/1e12:.2f}T tokens\")\n",
    "print(f\"Expected loss at {budget} PF-days from compute scaling fit: {L_budget:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e7d33",
   "metadata": {},
   "source": [
    "## 4) Assumptions & limitations\n",
    "1) **Collinearity of N and D:** In the provided table, dataset size and model size scale together (D≈100N). With only four points on a single frontier, it is not possible to reliably disentangle separate scaling exponents for model size and data size.\n",
    "\n",
    "2) **Compute proxy:** I treat PF-days as a consistent proxy for training compute and assume the compute definition is comparable across runs. In practice, hardware efficiency, sequence length, optimizer, and implementation details shift this relationship.\n",
    "\n",
    "3) **Extrapolation risk:** Predicting at 10B/1T requires extrapolating far beyond the observed compute range. Scaling laws are often approximately valid over ranges, but constants/exponents can change with architecture, data mixture, and optimization.\n",
    "\n",
    "4) **Single-metric view:** I model only final loss. Real deployments care about downstream task performance, calibration, safety, and robustness, which may not track loss monotonically.\n",
    "\n",
    "5) **Frontier optimality:** The allocation recommendation assumes the original runs are close to compute-optimal (a reasonable heuristic in scaling work), but without additional off-frontier data points (e.g., same compute with different N vs D), this cannot be proven."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
