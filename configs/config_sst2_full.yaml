model_name: "distilbert-base-uncased"
dataset_name: "sst2"
max_length: 128
batch_size: 8
learning_rate: 2e-5
weight_decay: 0.01
num_epochs: 3
seed: 42
early_stopping_patience: 2
device: "cpu"
output_dir: "./outputs/sst2_full"
logging_dir: "./runs"
num_labels: 2
augmentation:
  random_token_mask_prob: 0.0
  enable: false
ablation:
  freeze_base_model: false
  use_distilbert: true
checkpoint:
  save_every_n_steps: 0
  resume: false
  path: ""
use_fp16: false
gradient_accumulation_steps: 1
dataloader_num_workers: 2
dataloader_pin_memory: false
